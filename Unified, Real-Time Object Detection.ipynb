{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78dfbbcc-0f2e-462b-86b6-a07e4afbd97b",
   "metadata": {},
   "source": [
    "# <center> Unified, Real-time Object Detection </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba28404-4f5f-419b-a7a0-1a9270938b03",
   "metadata": {},
   "source": [
    "#### <center> Giảng viên hướng dẫn: TS. Hoàng Xuân Tùng </center>\n",
    "#### <center> Sinh viên: Lê Vũ Quang. Mã số: 19020020 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9543cf6-3890-43d9-a944-0124f4472993",
   "metadata": {},
   "source": [
    "## I. Giới thiệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff8155-dd33-4d23-ab89-2ec7a2993d99",
   "metadata": {},
   "source": [
    "Phát hiện đối tượng là kỹ thuật quan trọng trong lĩnh vực Thị Giác Máy Tính.\n",
    "\n",
    "Kỹ thuật này hướng đến giải quyết bài toán phát hiện được vị trí các đối tượng và phân loại của chúng trong các tấm ảnh và đoạn phim kỹ thuật số. Nhờ khả năng đó mà nhiều ứng dụng thực tế ra đời, giúp nâng cao hiệu quả trong sản xuất, lao động cũng như chất lượng cuộc sống con người.\n",
    "\n",
    "Một ví dụ trong y học phẫu thuật, rất khó cho bác sĩ có thể nhận biết nhanh chính xác được cơ quan nội tạng của bệnh nhân do sự đa dạng sinh học nên nội tạng của họ có thể không giống nhau hoàn toàn. Một trong những ứng dụng đã ra đời đó là mô hình phát hiện thận, nó có thể tìm ra vị trí của cơ quan thận ngay lập tức trong các bức ảnh chụp cắt lớp [[1]](#ref1).\n",
    "Một ứng dụng cụ thể khác được phát triển trong nông nghiệp là hệ thống thu hoạch thông minh, những con rô-bốt có thể nhìn và phát hiện được những trái cà chua chín, từ đó giúp những người nông dân trở nên đỡ vất vả hơn và đạt được hiệu quả tốt hơn trong khâu thu hoạch [[2]](#ref2).\n",
    "Bên cạnh đó, có thể kể đến như hệ thống xe tự hành của Tesla, hệ thống phát hiện vi phạm trong an toàn giao thông,...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d233017-2e0f-48da-b5ee-679139a7aa64",
   "metadata": {},
   "source": [
    "## II. Tổng quan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3892e-d080-4aab-acd7-127deff803da",
   "metadata": {},
   "source": [
    "Các hệ thống phát hiện đối tượng dựa trên các thuật toán học máy và học sâu. \n",
    "\n",
    "Ở những giai đoạn đầu có thể kể đến những mô hình deformable parts models (DPM). Mô hình được đề xuất bởi P. Felzenszwalb vào năm 2008. Hướng tiếp cận sử dụng cửa sổ trượt với đa dạng kích thước, đồng thời chạy các bộ phân lớp song song với việc chạy cửa sổ trượt trên toàn hình ảnh [[3]](#ref3). Lấy ví dụ trên bài toán phát hiện cà chua chín, rô-bốt thu hoạch có 1 bộ phân lớp để phát hiện hình ảnh đã cho có phải là quả cà chua chín hay không. Rô-bốt chụp một hình ảnh trước mặt nó, sau đó sẽ chọn một cửa sổ trượt với kích thước nào đó, cho cửa sổ trượt chạy trên toàn bộ hình ảnh, từ trái sang phải, từ trên xuống dưới. Với mỗi vị trí cửa sổ trượt, nó sẽ thực thi bộ phân lớp xem đó có phải là hình ảnh quả cà chua chín hay không. Rô-bốt có thể sẽ sử dụng các cửa sổ trượt với các kích thước khác nhau để tối ưu việc nhận biết.\n",
    "\n",
    "Hướng tiếp cận mới hơn là họ thuật toán Region-based Convolutional Neural Networks (R-CNN). R-CNN được giới thiệu lần đầu vào 2014 bởi Ross Girshick và các cộng sự ở UC Berkeley một trong những trung tâm nghiên cứu AI hàng đầu thế giới trong bài báo Rich feature hierarchies for accurate object detection and semantic segmentation [[4]](#ref4). R-CNN là một trong những ứng dụng nền móng đầu tiên của mạng nơ-ron tích chập trong các bài toán phát hiện đối tượng. Thay vì dùng cửa sổ trượt như DPM, R-CNN tạo và trích xuất các vùng đề xuất chứa vật thể được bao bởi các viền hình chữ nhật sử dụng kỹ thuật Selective Search [[4]](#ref4). Sau khi có các vùng đề xuất, công việc trích xuất đặc trưng và phân lớp đối tượng sẽ được thực hiện dựa trên một mạng CNN học sâu mang tên AlexNet [[5]](#ref5). Tuy nhiên R-CNN sau đó đã được nhận thấy còn nhiều hạn chế về tốc độ và độ chính xác, Fast R-CNN [[6]](#ref6) và Faster R-CNN [[7]](#ref7) lần lượt được ra đời vào các năm 2015 và 2016.\n",
    "\n",
    "Một lớp thuật toán rất phổ biến khác đó là YOLO [[8]](#ref8) (You only look once) cũng dựa trên một mạng nơ-ron, được đề xuất vào năm 2015 bởi Joseph Redmon và các cộng sự của mình, một strong số đó là Ross Girshick - người đã sáng tạo ra R-CNN. YOLO học khái quát hoá cách biểu diễn của đối tượng khá tốt. Ví dụ khi tập huấn luyện là dữ liệu hình ảnh về thiên nhiên, trong khi dữ liệu kiểm thử là tập ảnh nghệ thuật, hiệu quả của YOLO vượt xa các mô hình DPM và R-CNN. Tuy nhiên, độ chính xác của YOLO chưa đạt được như R-CNN, nó khó có thể dự đoán chính xác được vị trí của đối tượng, đặc biệt là các đối tượng nhỏ. Nhưng so về mặt tốc độc, YOLO nhanh hơn rất nhiều, thậm chí đạt được việc phát hiện đối tượng trong thời gian thực. Các cải tiến của YOLO lần lượt được đưa ra sau đó, cho đến nay đã có sự xuất hiện của YOLOv7 [[9]](#ref9). \n",
    "\n",
    "Bài viết sẽ tiếp tục trình bày chi tiết về YOLOv1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fe55f4-e660-41e1-bc76-bccda78107bc",
   "metadata": {},
   "source": [
    "## III. Chi tiết thuật toán YOLO V1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd9b77-1dc4-4cab-8ebe-f20fb5175cd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Tóm tắt lại bài toán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc058ac-90c6-499a-80c7-462964f08f7d",
   "metadata": {},
   "source": [
    "Đầu vào:\n",
    "  - I: ảnh đầu vào\n",
    "  - C: tập các lớp\n",
    "\n",
    "Đầu ra:\n",
    "  - {$r_{1}$, $r_{2}$, ..., $r_{m}$}: m boxes (bounding boxes) khoanh vùng vị trí m đối tượng được nhận diện\n",
    "  - {$l_{1}$, $l_{2}$, ..., $l_{m}$}: nhãn của m đối tượng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fd22eb-f815-43d4-873b-3c7b5c303460",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Phát hiện thống nhất (Unified Detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c731b1-37cc-49cd-a41c-0bcf4c39a087",
   "metadata": {},
   "source": [
    "<em> Các con số cụ thể như kích thước ảnh, số lượng anchor, kích thước tập nhãn,... sẽ được giả sử và sử dụng nhằm mục đích giúp bài viết cụ thể và dễ hình dung hơn. </em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa50efb-25db-4e7a-a8df-47b4568f7f28",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### a. Anchors và bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24efcae-6c75-4e40-9442-024ebaf61105",
   "metadata": {},
   "source": [
    "Ảnh đầu vào sẽ được thay đổi kích thước về 448 x 448.\n",
    "\n",
    "Chia ảnh thành 7 x 7 ô vuông nhỏ có kích thước bằng nhau. Mỗi ô vuông này gọi là 1 anchor.\n",
    "\n",
    "Một anchor gọi là chứa đối tượng nếu tâm của đối tượng rơi vào anchor đó. Đối tượng có thể xuất hiện ở các anchor khác trong ảnh nhưng tâm của đối tượng không rơi vào những anchor đó thì những anchor đó không gọi là chứa đối tượng. Hạn chế của YOLO là 1 anchor chỉ có thể chứa tối đa 1 đối tượng, điều này có thể cải tiến bằng cách tăng số lượng anchor.\n",
    "\n",
    "Với mỗi anchor, mục tiêu:\n",
    "- Dự đoán 2 bounding boxes cùng với xác suất bounding box chứa một đối tượng. Số lượng tham số cần dự đoán là $2 * (4 + 1)$.\n",
    "- Các xác suất thuộc các nhãn của đối tượng. Số lượng tham số cần dự đoán là $n$ kích thước tập nhãn.\n",
    "\n",
    "Giả sử trong bài toán này kích thước của tập nhãn là $n = 20$. \n",
    "\n",
    "Tổng số các tham số cần dự đoán cho 1 anchor là: $2 * 5 + 20 = 30$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd45934-45b7-44a0-a437-db2165729c19",
   "metadata": {},
   "source": [
    "Với mỗi bounding box $(x, y, w, h)$ trong đó $(x, y)$ là toạ độ góc trái trên của bounding box, $(w, h)$ tương ứng là chiều rộng, chiều cao của box. Ta sẽ encode bounding box như sau để dễ dàng cho việc tính toán:\n",
    "- $\\Delta x = (x - x_a)/64$\n",
    "- $\\Delta y = (y - y_a)/64$\n",
    "- $\\Delta w = w/448$\n",
    "- $\\Delta h = h/448$\n",
    "\n",
    "Trong đó $(x_a, y_a)$ là toạ độ góc trái trên của anchor, 64 là kích thước của anchor, 448 là kích thước của hình ảnh sau khi đã resize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003e221-aa08-4cb5-ab41-6d626a9892a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### b. Kiến trúc mạng lưới"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32650a6-261e-4253-802e-9e1d55d260ab",
   "metadata": {},
   "source": [
    "![YOLOv1 Network Architecture](https://miro.medium.com/max/1400/1*U07nDXlgshGNrt8MpBcZ0w.webp \"YOLOv1 Network Architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe59633-d216-43ac-b6f6-451540db1834",
   "metadata": {},
   "source": [
    "Ta sẽ xây dựng một mạng lưới sao cho ma trận đầu vào có kích thước 448 x 448 x 3 <em>(3 colors R, G, B)</em> và đầu ra có kích thước 7 x 7 x 30. Chi tiết về kiến trúc mạng lưới của YOLOv1 như ảnh trên. Ý tưởng tổng quan là từ ma trận đầu vào, đưa qua lần lượt các convolutional layers [[10]](#ref10) cùng với maxpool layers [[10]](#ref10) để được kết quả như ý."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2486b7cc-8ab9-474f-89c0-9dc710a26aaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### c. Xử lý ma trận đầu ra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c595926-cac3-4ead-a5d3-0e1a661407ea",
   "metadata": {},
   "source": [
    "Đây là quá trình từ ma trận đầu ra 7 x 7 x 30 ta có thể khoanh vùng và gán lớp cho các đối tượng lên trên bức ảnh đầu vào."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02e4fb1-a166-4039-8ea8-d22f0bebdba8",
   "metadata": {},
   "source": [
    "Xét 1 vector biểu diễn tham số được dự đoán của một anchor trong tổng số 7 x 7 vector:\n",
    "\n",
    "$[\\Delta x_1, \\Delta y_1, \\Delta w_1, \\Delta h_1, c_1, \\Delta x_2, \\Delta y_2, \\Delta w_2, \\Delta h_2, c_2, p_1, p_2, p_3, \\dots, p_{20}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac1422-1457-4acd-b0ad-1b323f12909b",
   "metadata": {},
   "source": [
    "Từ $(\\Delta x_i, \\Delta y_i, \\Delta w_i, \\Delta h_i)$ ta có thể suy ngược lại $(x_i, y_i, w_i, h_i)$ là thông số của bounding box:\n",
    "- $x_i = \\Delta x_i * 64 + x_a$\n",
    "- $y_i = \\Delta y_i * 64 + y_a$\n",
    "- $w_i = \\Delta w_i * 448$\n",
    "- $h_i = \\Delta h_i * 448$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468787e-a2cb-4214-b4e3-c4f6747a1cea",
   "metadata": {},
   "source": [
    "**Lớp của đối tượng**: $l = argmax(p_1, p_2, p_3, \\dots, p_{20})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b161813-6dd5-42ff-bcbc-c61a6f7b9192",
   "metadata": {},
   "source": [
    "Xác suất để bounding box chứa đối tượng thuộc lớp $l$ gọi là **class confidence**:\n",
    "- $\\hat{c_1} = c_1 * p$\n",
    "- $\\hat{c_2} = c_2 * p$\n",
    "\n",
    "Trong đó $p = max(p_1, p_2, p_3, \\dots, p_{20})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b38839-559f-4e9a-ae1b-0d7bdd363dc1",
   "metadata": {},
   "source": [
    "Từ những thông số thu được ở trên, ta tiếp tục thực hiện 2 bước để đạt được kết quả nhận diện cuối cùng:\n",
    "- Loại bỏ những bounding boxes có **class confidence** nhỏ hơn một ngưỡng được định nghĩa trước.\n",
    "- Áp dụng thuật toán Non-maximal Suppression (NMS) [[11]](#ref11) để lọc ra các bounding boxes cuối cùng. Vấn đề NMS giải quyết là với đầu ra của mô hình có thể sẽ cho nhiều bounding boxes cùng bao lấy một đối tượng trong khi với mỗi đối tượng ta chỉ cần 1 bounding box cho nó với sự chính xác cao nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c6d39-d09c-4220-aa35-443fcf878ffc",
   "metadata": {},
   "source": [
    "### 3. Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7aed2b-cf26-4bac-9dc9-9ec326e10e0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### a. Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96552a5a-5477-4eae-b367-1ea80e3ccd2d",
   "metadata": {},
   "source": [
    "Tập dữ liệu huấn luyện bao gồm dữ liệu vào là những tấm ảnh cùng với đầu ra là tập các bounding boxes của đối tượng và nhãn của chúng: $(\\hat{x}, \\hat{y}, \\hat{w}, \\hat{h}, \\hat{l})$.\n",
    "\n",
    "Từ đó ta sẽ sinh ra tập tham số mục tiêu của mỗi anchor:\n",
    "- Anchor chứa đối tượng (anchor mà tâm của các bounding boxes nói trên nằm trong): <br>\n",
    "$(\\Delta \\hat{x}, \\Delta \\hat{y}, \\Delta \\hat{w}, \\Delta \\hat{h}, \\hat{c}, \\hat{p}_1, \\hat{p}_2, \\dots, \\hat{p}_{20})$\n",
    "\n",
    "> Trong đó: <br>\n",
    "    $\\Delta \\hat{x} = (\\hat{x} - x_a)/64$ <br>\n",
    "    $\\Delta \\hat{y} = (\\hat{y} - y_a)/64$ <br>\n",
    "    $\\Delta \\hat{w} = \\hat{w}/448$ <br>\n",
    "    $\\Delta \\hat{h} = \\hat{h}/448$ <br>\n",
    "    $\\hat{c} = 1$ <br>\n",
    "    $\\hat{p}_{i = \\hat{l}} = 1$ và $\\hat{p}_{i != \\hat{l}} = 0$ <br>\n",
    "\n",
    "- Anchor không chứa đối tượng: $(0, 0, \\dots, 0)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb272b-a5da-4109-aa9b-544be8088b1b",
   "metadata": {},
   "source": [
    "*Lưu ý: tập $(\\Delta \\hat{x}, \\Delta \\hat{y}, \\Delta \\hat{w}, \\Delta \\hat{h})$ gọi là ground-truth box.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25d6353-a23f-4b65-9936-389d37272ba3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### b. Định nghĩa hàm mất mát"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96762c78-8a83-43be-a4c6-c71522276f85",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### b.1 Tổng mất mát L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800025e6-7171-472b-a496-c40be5fc4c9e",
   "metadata": {},
   "source": [
    "Được định nghĩa là tổng mất mát của S x S hay cụ thể ở đây là 7 x 7 anchors.\n",
    "\n",
    "*Thứ tự các anchor quy ước là trái sang phải, từ trên xuống dưới*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74abc74a-5108-49e7-abb4-78009531d793",
   "metadata": {},
   "source": [
    "$L = \\sum_{i = 1}^{S^2} 1^{obj}_i*L_{i,obj} + \\lambda_{no\\_obj} * \\sum_{i = 1}^{S^2} 1^{no\\_obj}_i*L_{i,no\\_obj}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9599cce6-ed75-46ed-a5ba-4346d5fd8ded",
   "metadata": {
    "tags": []
   },
   "source": [
    "Trong đó:\n",
    "- $1^{obj}_i = 1$ nếu anchor thứ i chứa đối tượng.\n",
    "- $1^{no\\_obj}_i = 1$ nếu anchor thứ i không chứa đối tượng.\n",
    "- $L_{i,obj}$ mất mát của anchor thứ i tính theo hàm mất mát của loại anchor chứa đối tượng.\n",
    "- $L_{i,no\\_obj}$ mất mát của anchor thứ i tính theo hàm mất mát của loại anchor không chứa đối tượng.\n",
    "- $\\lambda_{no\\_obj}$ thường được đặt giá trị là 0.5 để giảm sự quan trọng của những anchor không chứa đối tượng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a728b-d5fc-421f-88a3-9c8b2ee93981",
   "metadata": {},
   "source": [
    "Từ tổng mất mát $L$, ta sẽ sử dụng backpropagation [[10]](#ref10) để tối ưu mô hình, giảm thiểu mất mát."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501c8462-8207-4180-b38f-c28ebff4f980",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### b.2 Hàm mất mát $L_{i,obj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec6c0d7-acac-4fb1-8ddc-d97dcde0b61b",
   "metadata": {},
   "source": [
    "Được định nghĩa là tổng mất mát của ba loại dự đoán: dự đoán bounding box, dự đoán độ tin cậy và dự đoán nhãn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b64529b-e39e-41d6-bdc9-cd851d6c599e",
   "metadata": {},
   "source": [
    "$L_{i,obj} = \\lambda_{coord} * L^{box}_{i,obj} + L^{conf}_{i,obj} + L^{cls}_{i,obj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbac1a3-0817-48ed-a372-fc1477d938f4",
   "metadata": {},
   "source": [
    "Trong đó:\n",
    "- $\\lambda_{coord}$ thường được đặt là 50 để tăng độ quan trọng của việc dự đoán bounding box.\n",
    "- $L^{box}_{i,obj}$ là mất mát của dự đoán bounding box.\n",
    "- $L^{conf}_{i,obj}$ là mất mát của dự đoán độ tin cậy hay xác suất của việc liệu bounding box có chứa đối tượng nào đó hay không đã nói ở trên.\n",
    "- $L^{cls}_{i,obj}$ là mất mát của dự đoán nhãn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626b600a-dd61-4dd4-b342-75bab206b223",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### b.2.1 Hàm mất mát dự đoán bounding box $L^{box}_{i,obj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eea29d6-574f-451c-a321-b835eb9d9d49",
   "metadata": {},
   "source": [
    "$L^{box}_{i,obj} = (\\Delta x^*_i - \\Delta \\hat{x}_i)^2 + (\\Delta y^*_i - \\Delta \\hat{y}_i)^2 + (\\sqrt{\\Delta w^*_i} - \\sqrt{\\Delta\\hat{w}_i})^2 + (\\sqrt{\\Delta h^*_i} - \\sqrt{\\Delta\\hat{h}_i})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be4356a-2a3a-49f8-91ff-f2874feace27",
   "metadata": {},
   "source": [
    "Trong đó:\n",
    "- $(\\Delta \\hat{x}, \\Delta \\hat{y}, \\Delta \\hat{w}, \\Delta \\hat{h})$: ground-truth đã đề cập trên.\n",
    "- $(\\Delta x^*_i, \\Delta y^*_i, \\Delta w^*_i, \\Delta h^*_i)$: bounding box mà có chỉ số IoU [[12]](#ref12) với ground-truth lớn hơn trong 2 bounding boxes được dự đoán ở mỗi anchor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b12029d-d068-45cb-99c5-fb17edfca06b",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### b.2.2 Hàm mất mát dự đoán độ tin cậy $L^{conf}_{i,obj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba5e159-b519-4593-96cb-77168dc8f98c",
   "metadata": {},
   "source": [
    "$L^{conf}_{i,obj} = (c^*_i - \\hat{c}_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc42898-4168-4008-ab44-152e926a38b9",
   "metadata": {},
   "source": [
    "Trong đó:\n",
    "- $c^*_i$ là confidence của bounding box được đề cập khi tính $L^{box}_{i,obj}$ bên trên.\n",
    "- $\\hat{c}_i = 1.0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21573d32-c1c9-4d34-a15e-8a637d1fa99b",
   "metadata": {
    "tags": []
   },
   "source": [
    "###### b.2.3 Hàm mất mát dự đoán nhãn $L^{cls}_{i,obj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a17e8-666f-405f-b0c7-b083d688acc1",
   "metadata": {},
   "source": [
    "$L^{cls}_{i,obj} = \\sum_{c = 1}^{n=20} (p_{i,c} - \\hat{p}_{i,c})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec68e8-67b4-4762-871e-e5dd471aad96",
   "metadata": {
    "tags": []
   },
   "source": [
    "Trong đó:\n",
    "- $(p_{i,1}, p_{i,2}, \\dots, p_{i,20})$: xác suất dự đoán thuộc các lớp của đối tượng nằm trong anchor $i$.\n",
    "- $(\\hat{p}_{i,1}, \\hat{p}_{i,2}, \\dots, \\hat{p}_{i,20})$: nằm trong vector one-hot encoding đã trình bày trên."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00de1d4-1527-4b9c-a6c3-0e2e530d5a22",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### b.3 Hàm mất mát $L_{i,no\\_obj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b104c85b-ca9a-4536-8186-9a7a09c87841",
   "metadata": {},
   "source": [
    "$L_{i,no\\_obj} = \\sum_{j=1}^{B = 2} c^2_{i,j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3474ded4-a322-4bc1-8281-3e078602a9d0",
   "metadata": {},
   "source": [
    "Trong đó $(c_{i,1}, c_{i,2})$ lần lượt là confidence của 2 bounding boxes được dự đoán từ anchor $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee99bf4-016f-4387-ac64-61d243264d6f",
   "metadata": {},
   "source": [
    "## IV. Thí nghiệm và đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15661d83-5e42-44f1-988e-e7decc8c10a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc8005d7-cdee-4eda-9de4-e8cd03206b1a",
   "metadata": {},
   "source": [
    "## V. Kết luận"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc37416-05e4-4db7-a47e-dc3f6a2a6b72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77d28a9d-25fc-4371-8110-5ff70d639e48",
   "metadata": {},
   "source": [
    "## VI. Tài liệu tham khảo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f031514-a2ba-4dd5-b882-7ca0281a9f54",
   "metadata": {},
   "source": [
    "<a name=\"ref1\"> [[1] KIDNEY RECOGNITION IN CT USING YOLOV3 ](https://arxiv.org/pdf/1910.01268.pdf) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7bbf66-d28a-445f-9367-bfbee2896ca2",
   "metadata": {},
   "source": [
    "<a name=\"ref2\"> [[2] Tomato detection based on modified YOLOv3 framework ](https://www.nature.com/articles/s41598-021-81216-5) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eeedf2-9b84-4dd0-8041-d5b6e15f9b33",
   "metadata": {},
   "source": [
    "<a name=\"ref3\"> [[3] Object Detection with Discriminatively Trained Part Based Models ](https://cs.brown.edu/people/pfelzens/papers/lsvm-pami.pdf) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d561b5-71c7-41ca-8b09-7392519ba653",
   "metadata": {},
   "source": [
    "<a name=\"ref4\"> [[4] Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/pdf/1311.2524v5.pdf) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f43402-1227-4561-8949-32c71e48d937",
   "metadata": {},
   "source": [
    "<a name=\"ref5\"> [[5] ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c486a2b2-69ec-4633-8798-036ead5e03ae",
   "metadata": {},
   "source": [
    "<a name=\"ref6\"> [[6] Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5446bb-a8c1-4e56-8f0c-5b4f7d00b76e",
   "metadata": {},
   "source": [
    "<a name=\"ref7\"> [[7] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\n",
    "](https://arxiv.org/pdf/1506.01497.pdf) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56187c2d-639e-4293-a57f-7d27bce982a8",
   "metadata": {},
   "source": [
    "<a name=\"ref8\"> [[8] You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/pdf/1506.02640v5.pdf) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f11d1-c4f1-4ef5-baed-558c23567139",
   "metadata": {},
   "source": [
    "<a name=\"ref9\"> [[9] YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\n",
    "detectors](https://arxiv.org/pdf/2207.02696.pdf) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063ca97-f9dc-443e-9585-f01fcae3db33",
   "metadata": {},
   "source": [
    "<a name=\"ref10\"> [[10] An Introduction to Convolutional Neural Networks](https://arxiv.org/pdf/1511.08458.pdf) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b2528f-a7de-452a-8547-58e4d14abcdd",
   "metadata": {},
   "source": [
    "<a name=\"ref11\"> [[11] Learning non-maximum suppression](https://arxiv.org/pdf/1705.02950.pdf) </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a0a191-1658-415d-8bec-ba67e1152399",
   "metadata": {},
   "source": [
    "<a name=\"ref12\"> [[12] Intersection over Union (IoU) in Object Detection and Segmentation](https://learnopencv.com/intersection-over-union-iou-in-object-detection-and-segmentation/) </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d937d-ac56-41ec-b6c4-3f5aa06b1d16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
