{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe7cb0b-3a38-41da-8d30-4dc856895da9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d009b1953c70efcef74805dd07789ff",
     "grade": false,
     "grade_id": "cell-6983b47d44689971",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "_Cell chấm điểm_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3915c81f-ffcd-45f7-8883-d47989766e4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a94f353371a808e70497cf036115e8cb",
     "grade": false,
     "grade_id": "cell-5cd8999255ed307a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Yêu cầu bài tập lớn\n",
    "\n",
    "## Yêu cầu chung\n",
    "Chọn một chủ đề nghiên cứu; tìm, nghiên cứu tài liệu về chủ đề; và viết một báo cáo nghiên cứu trình bày lại chủ đề này. Chủ đề này có thể là: \n",
    "\n",
    "- Một ý tưởng nghiên cứu mới của sinh viên (Ví dụ chủ đề của luận văn thạc sỹ, hoặc một phần của LVTS)\n",
    "- Hoặc Một ý tưởng nghiên cứu đã có mà sinh viên tìm hiểu được hoặc quan tâm.\n",
    "- Phải liên quan đến lĩnh vực truyền thông đa phương tiện hoặc hệ thống đa phương tiện. Keywords tham khảo: \n",
    "    - Signal processing, \n",
    "    - image processing, \n",
    "    - Object detection, \n",
    "    - ML on video/image/audio data, \n",
    "    - computer vision, \n",
    "    - Compression, \n",
    "    - Compression sensing\n",
    "    - NLP, \n",
    "    - Text-to-speech, \n",
    "    - OCR,\n",
    "    - Machine translation\n",
    "    - Video Streaming\n",
    "    - Digital Right Management\n",
    "    - Resilient coding\n",
    "\n",
    "## Yêu cầu về định dạng\n",
    "Tham khảo cell mẫu dưới.\n",
    "Nội dung báo cáo có thể được chia ra thành nhiều cells. Các cells coding (thí nghiệm) và cell markdown (paragraph) thể đan xen\n",
    "\n",
    "## Tiêu chí chấm điểm\n",
    "Báo cáo sẽ được chấm tham khảo theo tiêu chí dưới đây:\n",
    "\n",
    "1. Introduction: \n",
    "    * Cần giới thiệu được chủ đề quan tâm. \n",
    "    * Giới thiệu được ý nghĩa, ứng dụng của chủ đề (chủ đề giải quyết vấn đề gì?)\n",
    "    * Các mức độ đánh giá: <b>Sâu sắc</b>, <b>Rõ ràng</b>, <b>Đầy đủ</b>, <b>có đề cập</b>\n",
    "2. Background and related work\n",
    "    * Giới thiệu được ngữ cảnh, tình huống, các vấn đề liên quan\n",
    "    * Trình bày cụ thể được bản chất vấn đề của chủ đề quan tâm\n",
    "    * Móc nối logic với phần Introduction\n",
    "    * Có ví dụ, minh hoạ định lượng để thể hiện vấn đề\n",
    "    * Các mức độ đánh giá: <b>Sâu sắc</b>, <b>Cụ thể, rõ ràng</b>, <b>có đề cập</b>\n",
    "3. Details of Your topic\n",
    "    * Trình bày được bản chất kỹ thuật của chủ đề liên quan\n",
    "    * Móc nối logic với các phần trước\n",
    "    * Có ví dụ, minh hoạ định lượng để giải thích\n",
    "    * Các mức độ đánh giá: <b>Sâu sắc</b>, <b>Cụ thể, rõ ràng</b>, <b>có đề cập</b>\n",
    "4. Analysis and evaluation\n",
    "    * Thiết kế, trình bày được thí nghiệm để phân tích đánh giá (một cách định lượng) liên quan đến chủ đề của báo cáo\n",
    "    * Thực hiện thí nghiệm, trình bày kết quả\n",
    "    * Các mức độ đánh giá: <b>Sâu sắc</b>, <b>Cụ thể, rõ ràng</b>, <b>có đề cập</b>\n",
    "5. Conclusion\n",
    "    * Tóm tắt lại nội dung và kết quả\n",
    "    * Hợp lý, thuyết phục\n",
    "    * Các mức độ đánh giá: <b>Sâu sắc</b>, <b>Cụ thể, rõ ràng</b>, <b>có đề cập</b>\n",
    "6. Yêu cầu khác:\n",
    "    * Có danh mục tài liệu tham khảo\n",
    "    * Thực hiện trích dẫn đầy đủ.\n",
    "    \n",
    "## Các chú ý khác\n",
    "Thông báo trước với thầy giáo về chủ đề của mình\n",
    "\n",
    "Không trích dẫn, tham khảo --> 0 điểm\n",
    "\n",
    "Đạo văn --> 0 điểm\n",
    "\n",
    "Không đăng ký các chủ đề trùng nhau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d2165-00c9-4a5d-9fe7-0e3481d1ba22",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "feab82932747334b88095719cef49120",
     "grade": false,
     "grade_id": "cell-578cd2920ddec17b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "-------------------------------------------------\n",
    "# Sample: Title of your report\n",
    "- Họ và tên: Nguyễn văn A\n",
    "- Số hiệu SV: 111111\n",
    "\n",
    "## Introduction\n",
    "< Your introduction >\n",
    "\n",
    "## Background and related work\n",
    "< Your content >\n",
    "\n",
    "## Details of Your topic of interest\n",
    "< Your content >\n",
    "\n",
    "## Analysis and evaluation\n",
    "< Your content >\n",
    "\n",
    "## Conclusion\n",
    "< Your content >\n",
    "\n",
    "------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda65143-8e08-4c41-a315-7855cbaa3f6b",
   "metadata": {},
   "source": [
    "# <center> Unified, Real-time Object Detection </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ff8e64-a631-4e42-ba25-57cad4f4125c",
   "metadata": {},
   "source": [
    "#### <center> Giảng viên hướng dẫn: TS. Hoàng Xuân Tùng </center>\n",
    "#### <center> Sinh viên: Lê Vũ Quang. Mã số: 19020020 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790aa5f-6e62-4894-9a1b-b41076390044",
   "metadata": {},
   "source": [
    "## I. Giới thiệu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39db462-4e4d-4af3-9e30-5cfc64554736",
   "metadata": {},
   "source": [
    "Phát hiện đối tượng là kỹ thuật quan trọng trong lĩnh vực Thị Giác Máy Tính.\n",
    "\n",
    "Kỹ thuật này hướng đến giải quyết bài toán phát hiện được vị trí các đối tượng và phân loại của chúng trong các tấm ảnh và đoạn phim kỹ thuật số. Nhờ khả năng đó mà nhiều ứng dụng thực tế ra đời, giúp nâng cao hiệu quả trong sản xuất, lao động cũng như chất lượng cuộc sống con người.\n",
    "\n",
    "Một ví dụ trong y học phẫu thuật, rất khó cho bác sĩ có thể nhận biết nhanh chính xác được cơ quan nội tạng của bệnh nhân do sự đa dạng sinh học nên nội tạng của họ có thể không giống nhau hoàn toàn. Một trong những ứng dụng đã ra đời đó là mô hình phát hiện thận, nó có thể tìm ra vị trí của cơ quan thận ngay lập tức trong các bức ảnh chụp cắt lớp [[1]](#ref1).\n",
    "Một ứng dụng cụ thể khác được phát triển trong nông nghiệp là hệ thống thu hoạch thông minh, những con rô-bốt có thể nhìn và phát hiện được những trái cà chua chín, từ đó giúp những người nông dân trở nên đỡ vất vả hơn và đạt được hiệu quả tốt hơn trong khâu thu hoạch [[2]](#ref2).\n",
    "Bên cạnh đó, có thể kể đến như hệ thống xe tự hành của Tesla, hệ thống phát hiện vi phạm trong an toàn giao thông,...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f6cd2-a1f6-471f-9cdf-990ce8fb653a",
   "metadata": {},
   "source": [
    "## II. Tổng quan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49414c38-4333-43d1-968c-baf274458b7f",
   "metadata": {},
   "source": [
    "Các hệ thống phát hiện đối tượng dựa trên các thuật toán học máy và học sâu. \n",
    "\n",
    "Ở những giai đoạn đầu có thể kể đến những mô hình deformable parts models (DPM). Mô hình được đề xuất bởi P. Felzenszwalb vào năm 2008. Hướng tiếp cận sử dụng cửa sổ trượt với đa dạng kích thước, đồng thời chạy các bộ phân lớp song song với việc chạy cửa sổ trượt trên toàn hình ảnh [[3]](#ref3). Lấy ví dụ trên bài toán phát hiện cà chua chín, rô-bốt thu hoạch có 1 bộ phân lớp để phát hiện hình ảnh đã cho có phải là quả cà chua chín hay không. Rô-bốt chụp một hình ảnh trước mặt nó, sau đó sẽ chọn một cửa sổ trượt với kích thước nào đó, cho cửa sổ trượt chạy trên toàn bộ hình ảnh, từ trái sang phải, từ trên xuống dưới. Với mỗi vị trí cửa sổ trượt, nó sẽ thực thi bộ phân lớp xem đó có phải là hình ảnh quả cà chua chín hay không. Rô-bốt có thể sẽ sử dụng các cửa sổ trượt với các kích thước khác nhau để tối ưu việc nhận biết.\n",
    "\n",
    "Hướng tiếp cận mới hơn là họ thuật toán Region-based Convolutional Neural Networks (R-CNN). R-CNN được giới thiệu lần đầu vào 2014 bởi Ross Girshick và các cộng sự ở UC Berkeley một trong những trung tâm nghiên cứu AI hàng đầu thế giới trong bài báo Rich feature hierarchies for accurate object detection and semantic segmentation [[4]](#ref4). R-CNN là một trong những ứng dụng nền móng đầu tiên của mạng nơ-ron tích chập trong các bài toán phát hiện đối tượng. Thay vì dùng cửa sổ trượt như DPM, R-CNN tạo và trích xuất các vùng đề xuất chứa vật thể được bao bởi các viền hình chữ nhật sử dụng kỹ thuật Selective Search [[4]](#ref4). Sau khi có các vùng đề xuất, công việc trích xuất đặc trưng và phân lớp đối tượng sẽ được thực hiện dựa trên một mạng CNN học sâu mang tên AlexNet [[5]](#ref5). Tuy nhiên R-CNN sau đó đã được nhận thấy còn nhiều hạn chế về tốc độ và độ chính xác, Fast R-CNN [[6]](#ref6) và Faster R-CNN [[7]](#ref7) lần lượt được ra đời vào các năm 2015 và 2016.\n",
    "\n",
    "Một lớp thuật toán rất phổ biến khác đó là YOLO [[8]](#ref8) (You only look once) cũng dựa trên một mạng nơ-ron, được đề xuất vào năm 2015 bởi Joseph Redmon và các cộng sự của mình, một strong số đó là Ross Girshick - người đã sáng tạo ra R-CNN. YOLO học khái quát hoá cách biểu diễn của đối tượng khá tốt. Ví dụ khi tập huấn luyện là dữ liệu hình ảnh về thiên nhiên, trong khi dữ liệu kiểm thử là tập ảnh nghệ thuật, hiệu quả của YOLO vượt xa các mô hình DPM và R-CNN. Tuy nhiên, độ chính xác của YOLO chưa đạt được như R-CNN, nó khó có thể dự đoán chính xác được vị trí của đối tượng, đặc biệt là các đối tượng nhỏ. Nhưng so về mặt tốc độc, YOLO nhanh hơn rất nhiều, thậm chí đạt được việc phát hiện đối tượng trong thời gian thực. Các cải tiến của YOLO lần lượt được đưa ra sau đó, cho đến nay đã có sự xuất hiện của YOLOv7 [[9]](#ref9). \n",
    "\n",
    "Bài viết sẽ tiếp tục trình bày chi tiết về YOLOv1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d59f5-89a9-427f-9e4b-3d4a1eed7872",
   "metadata": {},
   "source": [
    "## III. Chi tiết thuật toán YOLO V1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ba6c5-694d-4450-9885-4d32eb075c8e",
   "metadata": {},
   "source": [
    "### 1. Tóm tắt lại bài toán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70cd763-7648-4597-bfbf-a3681dc23347",
   "metadata": {},
   "source": [
    "Đầu vào:\n",
    "  - I: ảnh đầu vào\n",
    "  - C: tập các lớp\n",
    "\n",
    "Đầu ra:\n",
    "  - {$r_{1}$, $r_{2}$, ..., $r_{m}$}: m boxes (bounding boxes) khoanh vùng vị trí m đối tượng được nhận diện\n",
    "  - {$l_{1}$, $l_{2}$, ..., $l_{m}$}: nhãn của m đối tượng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e64b4d0-2a98-43dc-99b5-e8866129b47a",
   "metadata": {},
   "source": [
    "### 2. Phát hiện thống nhất (Unified Detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe632ddd-b4c1-4f60-aad0-c1a8e96a3a89",
   "metadata": {},
   "source": [
    "<em> Các con số cụ thể như kích thước ảnh, số lượng anchor, kích thước tập nhãn,... sẽ được giả sử và sử dụng nhằm mục đích giúp bài viết cụ thể và dễ hình dung hơn. </em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cee6b5-df80-4c75-9910-fc22c32e16c8",
   "metadata": {},
   "source": [
    "#### a. Anchors và bounding boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9a3396-97ab-4242-b36b-0cfb53a4c3ba",
   "metadata": {},
   "source": [
    "Ảnh đầu vào sẽ được thay đổi kích thước về 448 x 448.\n",
    "\n",
    "Chia ảnh thành 7 x 7 ô vuông nhỏ có kích thước bằng nhau. Mỗi ô vuông này gọi là 1 anchor.\n",
    "\n",
    "Một anchor gọi là chứa đối tượng nếu tâm của đối tượng rơi vào anchor đó. Đối tượng có thể xuất hiện ở các anchor khác trong ảnh nhưng tâm của đối tượng không rơi vào những anchor đó thì những anchor đó không gọi là chứa đối tượng. Hạn chế của YOLO là 1 anchor chỉ có thể chứa tối đa 1 đối tượng, điều này có thể cải tiến bằng cách tăng số lượng anchor.\n",
    "\n",
    "Với mỗi anchor, mục tiêu:\n",
    "- Dự đoán 2 bounding boxes cùng với xác suất bounding box chứa một đối tượng. Số lượng tham số cần dự đoán là $2 * (4 + 1)$.\n",
    "- Các xác suất thuộc các nhãn của đối tượng. Số lượng tham số cần dự đoán là $n$ kích thước tập nhãn.\n",
    "\n",
    "Giả sử trong bài toán này kích thước của tập nhãn là $n = 20$. \n",
    "\n",
    "Tổng số các tham số cần dự đoán cho 1 anchor là: $2 * 5 + 20 = 30$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aaf713-25c6-424f-aecc-55a65d5f3410",
   "metadata": {},
   "source": [
    "Với mỗi bounding box $(x, y, w, h)$ trong đó $(x, y)$ là toạ độ góc trái trên của bounding box, $(w, h)$ tương ứng là chiều rộng, chiều cao của box. Ta sẽ encode bounding box như sau để dễ dàng cho việc tính toán:\n",
    "- $\\Delta x = (x - x_a)/64$\n",
    "- $\\Delta y = (y - y_a)/64$\n",
    "- $\\Delta w = w/448$\n",
    "- $\\Delta h = h/448$\n",
    "\n",
    "Trong đó $(x_a, y_a)$ là toạ độ góc trái trên của anchor, 64 là kích thước của anchor, 448 là kích thước của hình ảnh sau khi đã resize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee14b74b-923b-4ebe-8626-15f356cd8b82",
   "metadata": {},
   "source": [
    "#### b. Kiến trúc mạng lưới"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401fd48d-e84e-4b58-8862-a3bf949f8f93",
   "metadata": {},
   "source": [
    "![YOLOv1 Network Architecture](https://miro.medium.com/max/1400/1*U07nDXlgshGNrt8MpBcZ0w.webp \"YOLOv1 Network Architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee31c484-0cf4-4db5-b9ba-c0af1ece2a82",
   "metadata": {},
   "source": [
    "Ta sẽ xây dựng một mạng lưới sao cho ma trận đầu vào có kích thước 448 x 448 x 3 <em>(3 colors R, G, B)</em> và đầu ra có kích thước 7 x 7 x 30. Chi tiết về kiến trúc mạng lưới của YOLOv1 như ảnh trên. Ý tưởng tổng quan là từ ma trận đầu vào, đưa qua lần lượt các convolutional layers [[10]](#ref10) cùng với maxpool layers [[10]](#ref10) để được kết quả như ý."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010b1eb5-dd33-4987-9cfc-5ed151faa65e",
   "metadata": {},
   "source": [
    "#### c. Xử lý ma trận đầu ra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaff9869-d0fb-41b0-8994-fe09e45525f6",
   "metadata": {},
   "source": [
    "Đây là quá trình từ ma trận đầu ra 7 x 7 x 30 ta có thể khoanh vùng và gán lớp cho các đối tượng lên trên bức ảnh đầu vào."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d3ae2a-ce91-4ebb-9f85-31113af1cbf5",
   "metadata": {},
   "source": [
    "Xét 1 vector biểu diễn tham số được dự đoán của một anchor trong tổng số 7 x 7 vector:\n",
    "\n",
    "$[\\Delta x_1, \\Delta y_1, \\Delta w_1, \\Delta h_1, c_1, \\Delta x_2, \\Delta y_2, \\Delta w_2, \\Delta h_2, c_2, p_1, p_2, p_3, \\dots, p_{20}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f49d52b-7e32-4059-9866-c15b59759606",
   "metadata": {},
   "source": [
    "Từ $(\\Delta x_i, \\Delta y_i, \\Delta w_i, \\Delta h_i)$ ta có thể suy ngược lại $(x_i, y_i, w_i, h_i)$ là thông số của bounding box:\n",
    "- $x_i = \\Delta x_i * 64 + x_a$\n",
    "- $y_i = \\Delta y_i * 64 + y_a$\n",
    "- $w_i = \\Delta w_i * 448$\n",
    "- $h_i = \\Delta h_i * 448$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577bc918-dfe4-4a7c-820b-d87ecf72d4c6",
   "metadata": {},
   "source": [
    "**Lớp của đối tượng**: $l = argmax(p_1, p_2, p_3, \\dots, p_{20})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee76b2f0-c644-4627-a583-2ddfd0114675",
   "metadata": {},
   "source": [
    "Xác suất để bounding box chứa đối tượng thuộc lớp $l$ gọi là **class confidence**:\n",
    "- $\\hat{c}_1 = c_1 * p$\n",
    "- $\\hat{c}_2 = c_2 * p$\n",
    "\n",
    "Trong đó $p = max(p_1, p_2, p_3, \\dots, p_{20})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b97466d-72d0-4541-b61a-c9bd367e8571",
   "metadata": {},
   "source": [
    "Từ những thông số thu được ở trên, ta tiếp tục thực hiện 2 bước để đạt được kết quả nhận diện cuối cùng:\n",
    "- Loại bỏ những bounding boxes có **class confidence** nhỏ hơn một ngưỡng được định nghĩa trước.\n",
    "- Áp dụng thuật toán Non-maximal Suppression (NMS) [[11]](#ref11) để lọc ra các bounding boxes cuối cùng. Vấn đề NMS giải quyết là với đầu ra của mô hình có thể sẽ cho nhiều bounding boxes cùng bao lấy một đối tượng trong khi với mỗi đối tượng ta chỉ cần 1 bounding box cho nó với sự chính xác cao nhất."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fac9fe-d191-4520-b5b5-e67dd23a0c96",
   "metadata": {},
   "source": [
    "### 3. Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349d64be-e3bc-4084-933b-c62e7e3fb9d4",
   "metadata": {},
   "source": [
    "#### a. Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ffb05e-10ab-474b-940e-7862faa7e20a",
   "metadata": {},
   "source": [
    "Tập dữ liệu huấn luyện bao gồm dữ liệu vào là những tấm ảnh cùng với đầu ra là tập các bounding boxes của đối tượng và nhãn của chúng: $(\\hat{x}, \\hat{y}, \\hat{w}, \\hat{h}, \\hat{l})$.\n",
    "\n",
    "Từ đó ta sẽ sinh ra tập tham số mục tiêu của mỗi anchor:\n",
    "- Anchor chứa đối tượng (anchor mà tâm của các bounding boxes nói trên nằm trong): <br>\n",
    "$(\\Delta \\hat{x}, \\Delta \\hat{y}, \\Delta \\hat{w}, \\Delta \\hat{h}, \\hat{c}, \\hat{p}_1, \\hat{p}_2, \\dots, \\hat{p}_{20})$\n",
    "\n",
    "> Trong đó: <br>\n",
    "    $\\Delta \\hat{x} = (\\hat{x} - x_a)/64$ <br>\n",
    "    $\\Delta \\hat{y} = (\\hat{y} - y_a)/64$ <br>\n",
    "    $\\Delta \\hat{w} = \\hat{w}/448$ <br>\n",
    "    $\\Delta \\hat{h} = \\hat{h}/448$ <br>\n",
    "    $\\hat{c} = 1$ <br>\n",
    "    $\\hat{p}_{i = \\hat{l}} = 1$ và $\\hat{p}_{i != \\hat{l}} = 0$ <br>\n",
    "\n",
    "- Anchor không chứa đối tượng: $(0, 0, \\dots, 0)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f53414-7d02-4571-b847-a724a3f3c0e0",
   "metadata": {},
   "source": [
    "*Lưu ý: tập $(\\Delta \\hat{x}, \\Delta \\hat{y}, \\Delta \\hat{w}, \\Delta \\hat{h})$ gọi là ground-truth box.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980adf51-417c-42d2-a6e6-0bdfeedb1765",
   "metadata": {},
   "source": [
    "#### b. Định nghĩa hàm mất mát"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a65e2b-5d7a-4cc5-98f8-2c5bec69bb8d",
   "metadata": {},
   "source": [
    "##### b.1 Tổng mất mát L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a453a552-d1f6-4ee9-b6f2-30fc2e49039c",
   "metadata": {},
   "source": [
    "Được định nghĩa là tổng mất mát của S x S hay cụ thể ở đây là 7 x 7 anchors.\n",
    "\n",
    "*Thứ tự các anchor quy ước là trái sang phải, từ trên xuống dưới*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb7abb2-9819-42cd-9d39-36f45b64e4d8",
   "metadata": {},
   "source": [
    "$L = \\sum_{i = 1}^{S^2} 1^{obj}_i*L_{i,obj} + \\lambda_{no\\_obj} * \\sum_{i = 1}^{S^2} 1^{no\\_obj}_i*L_{i,no\\_obj}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1979bbd2-a913-479f-88d2-4b6c1aa91070",
   "metadata": {},
   "source": [
    "Trong đó:\n",
    "- $1^{obj}_i = 1$ nếu anchor thứ i chứa đối tượng.\n",
    "- $1^{no\\_obj}_i = 1$ nếu anchor thứ i không chứa đối tượng.\n",
    "- $L_{i,obj}$ mất mát của anchor thứ i tính theo hàm mất mát của loại anchor chứa đối tượng.\n",
    "- $L_{i,no\\_obj}$ mất mát của anchor thứ i tính theo hàm mất mát của loại anchor không chứa đối tượng.\n",
    "- $\\lambda_{no\\_obj}$ thường được đặt giá trị là 0.5 để giảm sự quan trọng của những anchor không chứa đối tượng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e4771c-a840-48c6-bbe9-3b6bc811bc25",
   "metadata": {},
   "source": [
    "Từ tổng mất mát $L$, ta sẽ sử dụng backpropagation [[10]](#ref10) để tối ưu mô hình, giảm thiểu mất mát."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da5b60-4aad-41b8-be1e-016590038772",
   "metadata": {},
   "source": [
    "##### b.2 Hàm mất mát $L_{i,obj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9862e1-461f-4352-ae6f-f5aed673ff15",
   "metadata": {},
   "source": [
    "Được định nghĩa là tổng mất mát của ba loại dự đoán: dự đoán bounding box, dự đoán độ tin cậy và dự đoán nhãn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cde380-5001-414c-8d69-160b47df5da8",
   "metadata": {},
   "source": [
    "$L_{i,obj} = \\lambda_{coord} * L^{box}_{i,obj} + L^{conf}_{i,obj} + L^{cls}_{i,obj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4511059f-3037-4f56-af42-98763ff2b9d0",
   "metadata": {},
   "source": [
    "Trong đó:\n",
    "- $\\lambda_{coord}$ thường được đặt là 50 để tăng độ quan trọng của việc dự đoán bounding box.\n",
    "- $L^{box}_{i,obj}$ là mất mát của dự đoán bounding box.\n",
    "- $L^{conf}_{i,obj}$ là mất mát của dự đoán độ tin cậy hay xác suất của việc liệu bounding box có chứa đối tượng nào đó hay không đã nói ở trên.\n",
    "- $L^{cls}_{i,obj}$ là mất mát của dự đoán nhãn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5530849-99eb-4c40-90a0-439744c2cb58",
   "metadata": {},
   "source": [
    "###### b.2.1 Hàm mất mát dự đoán bounding box $L^{box}_{i,obj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a16df2-0182-437e-9786-0c0674aafe6b",
   "metadata": {},
   "source": [
    "$L^{box}_{i,obj} = (\\Delta x^*_i - \\Delta \\hat{x}_i)^2 + (\\Delta y^*_i - \\Delta \\hat{y}_i)^2 + (\\sqrt{\\Delta w^*_i} - \\sqrt{\\Delta\\hat{w}_i})^2 + (\\sqrt{\\Delta h^*_i} - \\sqrt{\\Delta\\hat{h}_i})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2e128-854d-4bf3-a4ea-b07358da0b65",
   "metadata": {},
   "source": [
    "Trong đó:\n",
    "- $(\\Delta \\hat{x}, \\Delta \\hat{y}, \\Delta \\hat{w}, \\Delta \\hat{h})$: ground-truth đã đề cập trên.\n",
    "- $(\\Delta x^*_i, \\Delta y^*_i, \\Delta w^*_i, \\Delta h^*_i)$: bounding box mà có chỉ số IoU [[12]](#ref12) với ground-truth lớn hơn trong 2 bounding boxes được dự đoán ở mỗi anchor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d93261-b733-4a72-bae0-7b38eeec39ce",
   "metadata": {},
   "source": [
    "###### b.2.2 Hàm mất mát dự đoán độ tin cậy $L^{conf}_{i,obj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406e0f73-290e-402d-8c12-52b1711b9894",
   "metadata": {},
   "source": [
    "$L^{conf}_{i,obj} = (c^*_i - \\hat{c}_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1099ad3-2262-48f7-8d98-2ba1bbf7dd7a",
   "metadata": {},
   "source": [
    "Trong đó:\n",
    "- $c^*_i$ là confidence của bounding box được đề cập khi tính $L^{box}_{i,obj}$ bên trên.\n",
    "- $\\hat{c}_i = 1.0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c3b9d-c464-48c8-a860-57ea84815381",
   "metadata": {},
   "source": [
    "###### b.2.3 Hàm mất mát dự đoán nhãn $L^{cls}_{i,obj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999bcf2f-cfb9-4308-9b64-0d5501112aa4",
   "metadata": {},
   "source": [
    "$L^{cls}_{i,obj} = \\sum_{c = 1}^{n=20} (p_{i,c} - \\hat{p}_{i,c})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbfe881-9e94-492d-9885-aeca5703c12f",
   "metadata": {},
   "source": [
    "Trong đó:\n",
    "- $(p_{i,1}, p_{i,2}, \\dots, p_{i,20})$: xác suất dự đoán thuộc các lớp của đối tượng nằm trong anchor $i$.\n",
    "- $(\\hat{p}_{i,1}, \\hat{p}_{i,2}, \\dots, \\hat{p}_{i,20})$: nằm trong vector one-hot encoding đã trình bày trên."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b365454-623d-4e40-a04a-1bf89132c8c0",
   "metadata": {},
   "source": [
    "##### b.3 Hàm mất mát $L_{i,no\\_obj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fcadef-ad8c-43c2-972e-1dcb407b2a9f",
   "metadata": {},
   "source": [
    "$L_{i,no\\_obj} = \\sum_{j=1}^{B = 2} c^2_{i,j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cb7c18-2b47-4494-8cf7-36198e71b138",
   "metadata": {},
   "source": [
    "Trong đó $(c_{i,1}, c_{i,2})$ lần lượt là confidence của 2 bounding boxes được dự đoán từ anchor $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919cd310-b708-4a64-9e5b-8a4f8396de53",
   "metadata": {},
   "source": [
    "## IV. Thí nghiệm và đánh giá"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a9809-95d0-4674-a500-bedca72871ec",
   "metadata": {},
   "source": [
    "Ta sẽ tiến hành xây dựng mô hình để xử lý bài toán nhận diện các hình vuông, chữ nhật, hình tròn.\n",
    "\n",
    "Thí nghiệm được tham khảo từ repo [yolo](https://github.com/pbcquoc/yolo) của tác giả pbcquoc.\n",
    "\n",
    "Tuy nhiên mã nguồn trong repo không còn chạy được do thư viện bị lỗi thời.\n",
    "\n",
    "Em đã chạy trên google colab [Multimedia-MidReport-19020020.ipynb](https://colab.research.google.com/drive/1ZO_PwGhOlRhlyZQnWWXwbWOl6gXGiaL8?usp=sharing) do không cài được một số thư viện trên int3305.\n",
    "\n",
    "Mã nguồn vẫn sẽ được trình bày dưới đây."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca42908-cc09-468f-8f35-274246c42afb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Cài đặt thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5f203d-593c-46d5-abd7-12d20731e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow\n",
    "! pip install --upgrade tf_slim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc49c5e-1b1a-4455-9164-088ff6f42969",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Import thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14db4bd-2824-429e-9cea-7a5fc69e1ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf_slim as slim\n",
    "import tf_slim.nets\n",
    "from tf_slim.nets import vgg \n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow.compat.v1 as v1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7e43e7-0961-4891-9866-2092b2903694",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Định nghĩa hằng số"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5fbe42-5da0-43e5-bb18-4df9da370ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 x 7 anchor\n",
    "cell_size = 7 \n",
    "# number of bounding boxes on each anchor\n",
    "box_per_cell = 2\n",
    "# image size\n",
    "img_size = 224\n",
    "# label set\n",
    "classes = {'circle':0, 'triangle':1,  'rectangle':2}\n",
    "nclass = len(classes)\n",
    "\n",
    "box_scale = 5.0\n",
    "noobject_scale = 0.5\n",
    "batch_size = 128\n",
    "\n",
    "epochs = 10\n",
    "# learning rate\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b861563-fb9f-4f06-91bf-38899b527d0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409b6c75-c17e-42e9-9b00-a6b860b864ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load():\n",
    "    labels = json.load(open('train/labels.json'))\n",
    "    # number of images\n",
    "    N = len(labels)\n",
    "    # matrix contains images\n",
    "    X = np.zeros((N, img_size, img_size, 3), dtype='uint8')\n",
    "    # matrix contains label of corresponding image\n",
    "    y = np.zeros((N,cell_size, cell_size, 5+nclass))\n",
    "    for idx, label in enumerate(labels):\n",
    "        img = cv2.imread(\"train/{}.png\".format(idx))\n",
    "        # normalize\n",
    "        X[idx] = img\n",
    "        for box in label['boxes']:\n",
    "            x1, y1 = box['x1'], box['y1']\n",
    "            x2, y2 = box['x2'], box['y2']\n",
    "            # one-hot vector\n",
    "            cl = [0]*len(classes)\n",
    "            cl[classes[box['class']]] = 1\n",
    "            # centor of boundary box\n",
    "            x_center, y_center, w, h = (x1+x2)/2.0, (y1+y2)/2.0, x2-x1, y2-y1\n",
    "            # anchor position\n",
    "            x_idx, y_idx = int(x_center/img_size*cell_size), int(y_center/img_size*cell_size)\n",
    "            # assign label\n",
    "            y[idx, y_idx, x_idx] = 1, x_center, y_center, w, h, *cl\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b92181-c786-4d86-98d0-4d53709c5341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tập dữ liệu\n",
    "! wget --quiet --no-check-certificate 'https://docs.google.com/uc?export=download&id=12sZLOe5VDvAqGHcjJh7mVmjB6HPeIEJh' -O train.zip\n",
    "! unzip -o -q train.zip\n",
    "! ls train | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e037d4ec-c86d-4ef4-acc9-1f6206ae54e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Chia tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e21146-af9b-40ac-9ada-e4c6215e9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ta chia tập dữ liệu thành tập train và test\n",
    "X, y = load()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec080327-1919-4649-ae9f-6e12ef09eba2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Định nghĩa mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a91d9c-5465-482e-9c64-4586040df5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26 layers mà ma trận đầu vào cần đi qua\n",
    "def vgg16(inputs, is_training):\n",
    "    \"\"\"định nghĩa CNN\n",
    "    Args:\n",
    "      inputs: 5-D tensor [batch_size, width, height, 3]\n",
    "    Return:\n",
    "      iou: 4-D tensor [batch_size, 7, 7, 5*nbox + nclass]\n",
    "    \"\"\"\n",
    "   \n",
    "    with v1.variable_scope(\"vgg_16\"):\n",
    "        with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "            net = slim.repeat(inputs, 2, slim.conv2d, 16, [3, 3], scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 32, [3, 3], scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 64, [3, 3], scope='conv3')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv4')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope='conv5')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool5')            \n",
    "            net = slim.conv2d(net, 512, [1, 1], scope='fc6')   \n",
    "            net = slim.conv2d(net, 13, [1, 1], activation_fn=None, scope='fc7')\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e3168d-297e-48cf-a840-4aa2df35bf9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Tính IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ece08-d1cb-49eb-8afc-bfdead0c4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2, scope='iou'):\n",
    "    \"\"\"calculate ious\n",
    "    Args:\n",
    "      boxes1: 5-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4]  ====> (x_center, y_center, w, h)\n",
    "      boxes2: 5-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL, 4] ===> (x_center, y_center, w, h)\n",
    "    Return:\n",
    "      iou: 4-D tensor [BATCH_SIZE, CELL_SIZE, CELL_SIZE, BOXES_PER_CELL]\n",
    "    \"\"\"\n",
    "    with v1.variable_scope(scope):\n",
    "        boxes1_t = tf.stack([boxes1[..., 0] - boxes1[..., 2] / 2.0,\n",
    "                             boxes1[..., 1] - boxes1[..., 3] / 2.0,\n",
    "                             boxes1[..., 0] + boxes1[..., 2] / 2.0,\n",
    "                             boxes1[..., 1] + boxes1[..., 3] / 2.0],\n",
    "                            axis=-1)\n",
    "\n",
    "        boxes2_t = tf.stack([boxes2[..., 0] - boxes2[..., 2] / 2.0,\n",
    "                             boxes2[..., 1] - boxes2[..., 3] / 2.0,\n",
    "                             boxes2[..., 0] + boxes2[..., 2] / 2.0,\n",
    "                             boxes2[..., 1] + boxes2[..., 3] / 2.0],\n",
    "                            axis=-1)\n",
    "\n",
    "        lu = tf.maximum(boxes1_t[..., :2], boxes2_t[..., :2])\n",
    "        rd = tf.minimum(boxes1_t[..., 2:], boxes2_t[..., 2:])\n",
    "\n",
    "        intersection = tf.maximum(0.0, rd - lu)\n",
    "        inter_square = intersection[..., 0] * intersection[..., 1]\n",
    "\n",
    "        square1 = boxes1[..., 2] * boxes1[..., 3]\n",
    "        square2 = boxes2[..., 2] * boxes2[..., 3]\n",
    "\n",
    "        union_square = tf.maximum(square1 + square2 - inter_square, 1e-10)\n",
    "\n",
    "    return tf.clip_by_value(inter_square / union_square, 0.0, 1.0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3aa473-199f-4c2b-95f8-ccdbf89e80e6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8. Định nghĩa hàm mất mát"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72686d6e-f0d9-4543-858a-19f1448f9822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_layer(predicts, labels, scope='loss_layer'):\n",
    "    \"\"\"calculate loss function\n",
    "    Args:\n",
    "      predicts: 4-D tensor [batch_size, 7, 7, 5*nbox+n_class] \n",
    "      labels: 4-D tensor [batch_size, 7, 7, 5+n_class]\n",
    "    Return:\n",
    "      loss: scalar\n",
    "    \"\"\"\n",
    "    with v1.variable_scope(scope):\n",
    "        offset = np.transpose(np.reshape(np.array(\n",
    "            [np.arange(cell_size)] * cell_size * box_per_cell),\n",
    "            (box_per_cell, cell_size, cell_size)), (1, 2, 0))\n",
    "        offset = offset[None, :]\n",
    "        offset = tf.constant(offset, dtype=tf.float32)\n",
    "        offset_tran = tf.transpose(offset, (0, 2, 1, 3))\n",
    "        predict_object = predicts[..., :box_per_cell]\n",
    "        predict_box_offset = tf.reshape(predicts[...,box_per_cell:5*box_per_cell], (-1, cell_size, cell_size, box_per_cell, 4))\n",
    "        predict_class = predicts[...,5*box_per_cell:]\n",
    "        predict_normalized_box = tf.stack(\n",
    "                                    [(predict_box_offset[..., 0] + offset) / cell_size,\n",
    "                                     (predict_box_offset[..., 1] + offset_tran) / cell_size,\n",
    "                                     tf.square(predict_box_offset[..., 2]),\n",
    "                                    tf.square(predict_box_offset[..., 3])], axis=-1)\n",
    "\n",
    "        true_object = labels[..., :1]\n",
    "        true_box = tf.reshape(labels[..., 1:5], (-1, cell_size, cell_size, 1, 4))\n",
    "        \n",
    "        true_normalized_box = tf.tile(true_box, (1, 1, 1, box_per_cell, 1))/img_size\n",
    "        true_class = labels[..., 5:]\n",
    "        \n",
    "        true_box_offset =  tf.stack(\n",
    "                                    [true_normalized_box[..., 0] * cell_size - offset,\n",
    "                                     true_normalized_box[..., 1] * cell_size - offset_tran,\n",
    "                                     tf.sqrt(true_normalized_box[..., 2]),\n",
    "                                     tf.sqrt(true_normalized_box[..., 3])], axis=-1)\n",
    "        \n",
    "        predict_iou = compute_iou(true_normalized_box, predict_normalized_box)\n",
    "        object_mask = tf.reduce_max(predict_iou, 3, keepdims=True)  \n",
    "        iou_metric = tf.reduce_mean(tf.reduce_sum(object_mask, axis=[1,2,3])/tf.reduce_sum(true_object, axis=[1,2,3]))        \n",
    "        object_mask = tf.cast((predict_iou>=object_mask), tf.float32)*true_object\n",
    "        noobject_mask = tf.ones_like(object_mask) - object_mask        \n",
    "        class_delta = true_object*(predict_class - true_class)\n",
    "        class_loss = tf.reduce_mean(tf.reduce_sum(tf.square(class_delta), axis=[1,2,3]), name='class_loss')\n",
    "        object_delta = object_mask*(predict_object - predict_iou)\n",
    "        object_loss = tf.reduce_mean(tf.reduce_sum(tf.square(object_delta), axis=[1,2,3]), name='object_loss')\n",
    "        noobject_delta = noobject_mask*predict_object\n",
    "        noobject_loss = tf.reduce_mean(tf.reduce_sum(tf.square(noobject_delta), axis=[1,2,3]), name='noobject_loss')\n",
    "        box_mask = tf.expand_dims(object_mask, 4)\n",
    "        box_delta = box_mask*(predict_box_offset - true_box_offset)\n",
    "        box_loss = tf.reduce_mean(tf.reduce_sum(tf.square(box_delta), axis=[1,2,3]), name='box_loss')\n",
    "        loss = 0.5*class_loss + object_loss + 0.1*noobject_loss + 10*box_loss\n",
    "        return loss, iou_metric, predict_object, predict_class, predict_normalized_box"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d8dc3-6bfa-463c-b338-be9a3e0e82f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 9. Biên dịch Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008bffc-040e-4068-8ad7-ad2190522860",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():    \n",
    "    images = v1.placeholder(\"float\", [None, img_size, img_size, 3], name=\"input\")\n",
    "    labels = v1.placeholder('float', [None, cell_size, cell_size, 8], name='label')\n",
    "    is_training = v1.placeholder(tf.bool)\n",
    "\n",
    "    logits = vgg16(images, is_training)\n",
    "    loss, iou_metric, predict_object, predict_class, predict_normalized_box = loss_layer(logits, labels)\n",
    "    \n",
    "    optimizer = v1.train.AdamOptimizer(lr)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed1565-4c3a-44a7-abe2-b8ee733d753a",
   "metadata": {},
   "source": [
    "## 10. Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319630bc-12eb-444e-b1fa-b81d4b420eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with v1.Session(graph=graph) as sess:\n",
    "    sess.run(v1.global_variables_initializer())\n",
    "  \n",
    "    saver = v1.train.Saver(max_to_keep=1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        for batch in range(len(X_train)//batch_size):\n",
    "            X_batch = X_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            y_batch = y_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            train_total_loss, train_iou_m,_ = sess.run([loss, iou_metric, train_op], {images:X_batch, labels:y_batch, is_training:True})            \n",
    "        end_time = time.time()\n",
    "        \n",
    "        val_loss = []\n",
    "        val_iou_ms = []\n",
    "        for batch in range(len(X_test)//batch_size):\n",
    "            val_X_batch = X_test[batch*batch_size:(batch+1)*batch_size]\n",
    "            val_y_batch = y_test[batch*batch_size:(batch+1)*batch_size]\n",
    "            total_val_loss, val_iou_m, val_predict_object, val_predict_class, val_predict_normalized_box = sess.run([loss, iou_metric, predict_object, predict_class, predict_normalized_box], \n",
    "                                                 {images:val_X_batch, labels:val_y_batch, is_training:False})\n",
    "            val_loss.append(total_val_loss)\n",
    "            val_iou_ms.append(val_iou_m)\n",
    "            \n",
    "        saver.save(sess, './model/yolo', global_step=epoch)\n",
    "        print('epoch: {} - running_time: {:.0f}s - train_loss: {:.3f} - train_iou: {:.3f} - val_loss: {:.3f} - val_iou: {:.3f}'.format(epoch, end_time - start_time, train_total_loss, train_iou_m, np.mean(val_loss), np.mean(val_iou_ms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d0639c-d91f-4feb-8a79-67927dfcc255",
   "metadata": {},
   "source": [
    "## 11. Hiển thị kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d0c66a-b8a5-4b42-be64-e414d30882be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      box1: [center_x, center_y, w, h] \n",
    "      box2: [center_x, center_y, w, h] \n",
    "    Return:\n",
    "      iou: iou\n",
    "    \"\"\"\n",
    "    tb = min(box1[0] + 0.5 * box1[2], box2[0] + 0.5 * box2[2]) - \\\n",
    "        max(box1[0] - 0.5 * box1[2], box2[0] - 0.5 * box2[2])\n",
    "    lr = min(box1[1] + 0.5 * box1[3], box2[1] + 0.5 * box2[3]) - \\\n",
    "        max(box1[1] - 0.5 * box1[3], box2[1] - 0.5 * box2[3])\n",
    "    inter = 0 if tb < 0 or lr < 0 else tb * lr\n",
    "    return inter / (box1[2] * box1[3] + box2[2] * box2[3] - inter)\n",
    "    \n",
    "def interpret_output(predict_object, predict_class, predict_normalized_box):\n",
    "    predict_box= predict_normalized_box*img_size\n",
    "    predict_object = np.expand_dims(predict_object, axis=-1)\n",
    "    predict_class = np.expand_dims(predict_class, axis=-2)\n",
    "    class_probs = predict_object*predict_class\n",
    "    filter_mat_probs = np.array(class_probs >= 0.2, dtype='bool')\n",
    "    filter_mat_boxes = np.nonzero(filter_mat_probs)\n",
    "    boxes_filtered = predict_box[filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]\n",
    "    class_probs_filtered = class_probs[filter_mat_probs]    \n",
    "    classes_num_filtered = np.argmax(\n",
    "        filter_mat_probs, axis=3)[\n",
    "        filter_mat_boxes[0], filter_mat_boxes[1], filter_mat_boxes[2]]\n",
    "    argsort = np.array(np.argsort(class_probs_filtered))[::-1]\n",
    "    boxes_filtered = boxes_filtered[argsort]\n",
    "    class_probs_filtered = class_probs_filtered[argsort]\n",
    "    classes_num_filtered = classes_num_filtered[argsort]\n",
    "\n",
    "    for i in range(len(boxes_filtered)):\n",
    "        if class_probs_filtered[i] == 0:\n",
    "            continue\n",
    "        for j in range(i + 1, len(boxes_filtered)):\n",
    "            if iou(boxes_filtered[i], boxes_filtered[j]) > 0.5:\n",
    "                class_probs_filtered[j] = 0.0\n",
    "                \n",
    "    filter_iou = np.array(class_probs_filtered > 0.0, dtype='bool')\n",
    "    boxes_filtered = boxes_filtered[filter_iou]\n",
    "    class_probs_filtered = class_probs_filtered[filter_iou]\n",
    "    classes_num_filtered = classes_num_filtered[filter_iou]\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(boxes_filtered)):\n",
    "        result.append(\n",
    "            [classes_num_filtered[i],\n",
    "             boxes_filtered[i][0],\n",
    "             boxes_filtered[i][1],\n",
    "             boxes_filtered[i][2],\n",
    "             boxes_filtered[i][3],\n",
    "             class_probs_filtered[i]])\n",
    "\n",
    "    return result\n",
    "\n",
    "def draw_result(img, result):\n",
    "    plt.figure(figsize=(10,10), dpi=40)\n",
    "    img = np.pad(img, [(50,50), (50,50), (0,0)], mode='constant', constant_values=255)\n",
    "    for i in range(len(result)):\n",
    "        x = int(result[i][1])+50\n",
    "        y = int(result[i][2])+50\n",
    "        w = int(result[i][3] / 2)\n",
    "        h = int(result[i][4] / 2)\n",
    "        cv2.rectangle(img, (x - w, y - h), (x + w, y + h), (231, 76, 60), 2)\n",
    "        cv2.rectangle(img, (x - w, y - h - 20),\n",
    "                      (x -w + 50, y - h), (46, 204, 113), -1)\n",
    "        cv2.putText(\n",
    "            img, '{} : {:.2f}'.format(result[i][0] ,result[i][5]),\n",
    "            (x - w + 5, y - h - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.3,\n",
    "            (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d54958-311f-4570-9ec4-879a35c35d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 10\n",
    "result = interpret_output(val_predict_object[img_idx], val_predict_class[img_idx], val_predict_normalized_box[img_idx])\n",
    "draw_result(val_X_batch[img_idx]*255, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb049858-2aad-4247-a8b8-59ea95576974",
   "metadata": {},
   "source": [
    "![](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133db5e5-b619-42f9-af78-b84c2810ff72",
   "metadata": {},
   "source": [
    "## 12. Đánh giá"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1692e48a-62b6-4c53-97d3-8328ccbf5c29",
   "metadata": {},
   "source": [
    "Với sự hỗ trợ của các thư viện mạnh như Tensorflow thì việc cài đặt thuật toán không hề quá khó khăn.\n",
    "Tuy nhiên, để huấn luyện được mô hình thì cần lượng tài nguyên rất lớn.\n",
    "\n",
    "Sau khoảng 100 epochs, mô hình có thể đem lại 0.9 IOU trên tập dữ liệu test gồm 5000 mẫu dữ liệu [[13]](#ref13).\n",
    "\n",
    "Như vậy kết quả đầu ra của mô hình đem lại khá tốt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba044b1-2ce4-4c8f-ba71-0468984f19b9",
   "metadata": {},
   "source": [
    "## V. Kết luận"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3809f0-002a-491c-87e8-73c41c374bc3",
   "metadata": {},
   "source": [
    "Bài viết này đã giới thiệu khái quát một số thuật toán về nhận diện đối tượng, và đi sâu vào thuật toán YOLOv1 với phần trình bày về tư tưởng thuật toán, kiến trúc mạng lưới, cùng với cách huấn luyện mô hình. Bên cạnh đó, thí nghiệm cài đặt thuật toán YOLOv1 cũng được đưa ra nhằm mục đích cố gắng giúp người đọc hình dung một cách sắc nét về YOLOv1.\n",
    "\n",
    "YOLOv1 đã ra đời cách thời điểm hiện tại một thời gian khá lâu, cho đến nay YOLO đã được phát triển tới v7. Mô hình YOLOv1 được xây dựng một cách đơn giản và có thể được huấn luyện trên toàn bộ tấm ảnh. So với các mô hình khác thời điểm đó, YOLOv1 vẫn đem lại hiệu quả khá tốt với tốc độ vượt trội là tiền đề để nghiên cứu và phát triển các thuật toán ngày càng tốt hơn trong lĩnh vực Computer Vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa245b7-96d7-403f-b401-4d6cafdcd80b",
   "metadata": {},
   "source": [
    "## VI. Tài liệu tham khảo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4117b-e84f-49d2-b932-93a0726d3f42",
   "metadata": {},
   "source": [
    "<a name=\"ref1\"> [[1] KIDNEY RECOGNITION IN CT USING YOLOV3 ](https://arxiv.org/pdf/1910.01268.pdf) </a>\n",
    "\n",
    "<a name=\"ref2\"> [[2] Tomato detection based on modified YOLOv3 framework ](https://www.nature.com/articles/s41598-021-81216-5) </a>\n",
    "\n",
    "<a name=\"ref3\"> [[3] Object Detection with Discriminatively Trained Part Based Models ](https://cs.brown.edu/people/pfelzens/papers/lsvm-pami.pdf) </a>\n",
    "\n",
    "<a name=\"ref4\"> [[4] Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/pdf/1311.2524v5.pdf) </a>\n",
    "\n",
    "<a name=\"ref5\"> [[5] ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) </a>\n",
    "\n",
    "<a name=\"ref6\"> [[6] Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf) </a>\n",
    "\n",
    "<a name=\"ref7\"> [[7] Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks\n",
    "](https://arxiv.org/pdf/1506.01497.pdf) </a>\n",
    "\n",
    "<a name=\"ref8\"> [[8] You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/pdf/1506.02640v5.pdf) </a>\n",
    "\n",
    "<a name=\"ref9\"> [[9] YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\n",
    "detectors](https://arxiv.org/pdf/2207.02696.pdf) </a>\n",
    "\n",
    "<a name=\"ref10\"> [[10] An Introduction to Convolutional Neural Networks](https://arxiv.org/pdf/1511.08458.pdf) </a>\n",
    "\n",
    "<a name=\"ref11\"> [[11] Learning non-maximum suppression](https://arxiv.org/pdf/1705.02950.pdf) </a>\n",
    "\n",
    "<a name=\"ref12\"> [[12] Intersection over Union (IoU) in Object Detection and Segmentation](https://learnopencv.com/intersection-over-union-iou-in-object-detection-and-segmentation/) </a>\n",
    "\n",
    "<a name=\"ref13\"> [[13] Tìm Hiểu YOLO cho Object Detection](https://github.com/pbcquoc/yolo#k%E1%BA%BFt-qu%E1%BA%A3-c%E1%BB%A7a-m%C3%B4-h%C3%ACnh) </a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
